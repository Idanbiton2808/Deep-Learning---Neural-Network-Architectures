{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0057a775",
   "metadata": {},
   "source": [
    "# Digit Detection Project-DL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fb6378a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8e8502b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = datasets.MNIST(\n",
    "    root = 'data',\n",
    "    train = True,\n",
    "    transform = ToTensor(),\n",
    "    download = True\n",
    ")\n",
    "\n",
    "test_data = datasets.MNIST(\n",
    "    root = 'data',\n",
    "    train = False,\n",
    "    transform = ToTensor(),\n",
    "    download = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e09f926c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset MNIST\n",
       "    Number of datapoints: 60000\n",
       "    Root location: data\n",
       "    Split: Train\n",
       "    StandardTransform\n",
       "Transform: ToTensor()"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2003f7e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10000, 28, 28])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6c5690e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 60000\n",
      "Test set size: 10000\n"
     ]
    }
   ],
   "source": [
    "print(\"Training set size:\", len(train_data))\n",
    "print(\"Test set size:\", len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f64ab7f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaders = {\n",
    "    'train' : DataLoader(train_data,\n",
    "                        batch_size = 100,\n",
    "                        shuffle = True,\n",
    "                        num_workers = 1),\n",
    "    \n",
    "    'test' : DataLoader(test_data,\n",
    "                        batch_size = 100,\n",
    "                        shuffle = True,\n",
    "                        num_workers = 1),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3edd525f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': <torch.utils.data.dataloader.DataLoader at 0x2b2783e6e90>,\n",
       " 'test': <torch.utils.data.dataloader.DataLoader at 0x2b2781a77d0>}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11a4e28b",
   "metadata": {},
   "source": [
    "# nn Model & Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b202aa92",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "25f3ba81",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size = 5)  \n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size = 5) \n",
    "        self.conv2_drop = nn.Dropout2d() # Regulazation layer\n",
    "        self.fc1 = nn.Linear(320, 50)  # Fully connected layer with 64*5*5 input features and 128 output features\n",
    "        self.fc2 = nn.Linear(50, 10)  # Output layer with 128 input features (from previous layer) and 10 output features\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x),2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)),2))\n",
    "        x = x.view(-1,320)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x , training=self.training) \n",
    "        x = self.fc2(x)\n",
    "         \n",
    "        return F.softmax(x, dim=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64959242",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "709ab605",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = Net().to(device)  # Assuming Net is your neural network model class\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(loaders['train']):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()  # set the gradient to zero for each batch\n",
    "        output = model(data)  # make the prediction\n",
    "        loss = loss_fn(output, target)  # compare the output and the actual digit and calculate the loss\n",
    "        loss.backward()\n",
    "        optimizer.step()  # optimizing by the backpropagation\n",
    "        if batch_idx % 20 == 0:\n",
    "            print(f'Train Epoch: {epoch} [{batch_idx * len(data)} / {len(loaders[\"train\"].dataset)} ({100. * batch_idx / len(loaders[\"train\"]):.0f}%)]\\t{loss.item():.6f}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "aa400c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ThreeLayerConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ThreeLayerConvNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.fc1 = nn.Linear(64 * 3 * 3, 128)  # Adjusted the input size for the fully connected layer\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv3(x), 2))\n",
    "        x = x.view(-1, 64 * 3 * 3)  # Adjusted the size based on the output size of the last convolutional layer\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return F.softmax(x, dim=1)\n",
    "\n",
    "# Instantiate the ThreeLayerConvNet model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "new_model = ThreeLayerConvNet().to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "32f5b1ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in loaders['test']:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += loss_fn(output, target).item()  # Accumulating the test loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "    test_loss /= len(loaders['test'].dataset)\n",
    "    print(f'\\nTest set: Average loss: {test_loss:.4f}, Accuracy: {correct} / {len(loaders[\"test\"].dataset)} ({100. * correct / len(loaders[\"test\"].dataset):.0f}%)\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1503c8e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0 / 60000 (0%)]\t2.304464\n",
      "Train Epoch: 1 [2000 / 60000 (3%)]\t2.292096\n",
      "Train Epoch: 1 [4000 / 60000 (7%)]\t2.169569\n",
      "Train Epoch: 1 [6000 / 60000 (10%)]\t2.012673\n",
      "Train Epoch: 1 [8000 / 60000 (13%)]\t1.901926\n",
      "Train Epoch: 1 [10000 / 60000 (17%)]\t1.784625\n",
      "Train Epoch: 1 [12000 / 60000 (20%)]\t1.770032\n",
      "Train Epoch: 1 [14000 / 60000 (23%)]\t1.739292\n",
      "Train Epoch: 1 [16000 / 60000 (27%)]\t1.773075\n",
      "Train Epoch: 1 [18000 / 60000 (30%)]\t1.673612\n",
      "Train Epoch: 1 [20000 / 60000 (33%)]\t1.684339\n",
      "Train Epoch: 1 [22000 / 60000 (37%)]\t1.660591\n",
      "Train Epoch: 1 [24000 / 60000 (40%)]\t1.677039\n",
      "Train Epoch: 1 [26000 / 60000 (43%)]\t1.640893\n",
      "Train Epoch: 1 [28000 / 60000 (47%)]\t1.689652\n",
      "Train Epoch: 1 [30000 / 60000 (50%)]\t1.573316\n",
      "Train Epoch: 1 [32000 / 60000 (53%)]\t1.628314\n",
      "Train Epoch: 1 [34000 / 60000 (57%)]\t1.626811\n",
      "Train Epoch: 1 [36000 / 60000 (60%)]\t1.604641\n",
      "Train Epoch: 1 [38000 / 60000 (63%)]\t1.683629\n",
      "Train Epoch: 1 [40000 / 60000 (67%)]\t1.607831\n",
      "Train Epoch: 1 [42000 / 60000 (70%)]\t1.681027\n",
      "Train Epoch: 1 [44000 / 60000 (73%)]\t1.657044\n",
      "Train Epoch: 1 [46000 / 60000 (77%)]\t1.628995\n",
      "Train Epoch: 1 [48000 / 60000 (80%)]\t1.662889\n",
      "Train Epoch: 1 [50000 / 60000 (83%)]\t1.612252\n",
      "Train Epoch: 1 [52000 / 60000 (87%)]\t1.634650\n",
      "Train Epoch: 1 [54000 / 60000 (90%)]\t1.660123\n",
      "Train Epoch: 1 [56000 / 60000 (93%)]\t1.583584\n",
      "Train Epoch: 1 [58000 / 60000 (97%)]\t1.589702\n",
      "\n",
      "Test set: Average loss: 0.0153, Accuracy: 9354 / 10000 (94%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1,2):\n",
    "    train(epoch)\n",
    "    test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b13cd58a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet3x3MaxPool(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNet3x3MaxPool, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.fc1 = nn.Linear(64 * 3 * 3, 128)  # Adjusted the input size for the fully connected layer\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv3(x), 2))\n",
    "        x = x.view(-1, 64 * 3 * 3)  # Adjusted the size based on the output size of the last convolutional layer\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return F.softmax(x, dim=1)\n",
    "\n",
    "new_model = ConvNet3x3MaxPool().to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b66f425d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "    losses = []  # List to store training losses for each epoch\n",
    "    for batch_idx, (data, target) in enumerate(loaders['train']):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = loss_fn(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses.append(loss.item())  # Append the training loss for this batch\n",
    "        if batch_idx % 20 == 0:\n",
    "            print(f'Train Epoch: {epoch} [{batch_idx * len(data)} / {len(loaders[\"train\"].dataset)} ({100. * batch_idx / len(loaders[\"train\"]):.0f}%)]\\t{loss.item():.6f}')\n",
    "    return losses\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f42eb25d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0 / 60000 (0%)]\t1.638490\n",
      "Train Epoch: 1 [2000 / 60000 (3%)]\t1.547747\n",
      "Train Epoch: 1 [4000 / 60000 (7%)]\t1.596798\n",
      "Train Epoch: 1 [6000 / 60000 (10%)]\t1.589628\n",
      "Train Epoch: 1 [8000 / 60000 (13%)]\t1.639228\n",
      "Train Epoch: 1 [10000 / 60000 (17%)]\t1.556213\n",
      "Train Epoch: 1 [12000 / 60000 (20%)]\t1.577391\n",
      "Train Epoch: 1 [14000 / 60000 (23%)]\t1.556094\n",
      "Train Epoch: 1 [16000 / 60000 (27%)]\t1.602403\n",
      "Train Epoch: 1 [18000 / 60000 (30%)]\t1.605165\n",
      "Train Epoch: 1 [20000 / 60000 (33%)]\t1.579773\n",
      "Train Epoch: 1 [22000 / 60000 (37%)]\t1.561525\n",
      "Train Epoch: 1 [24000 / 60000 (40%)]\t1.657276\n",
      "Train Epoch: 1 [26000 / 60000 (43%)]\t1.566274\n",
      "Train Epoch: 1 [28000 / 60000 (47%)]\t1.532547\n",
      "Train Epoch: 1 [30000 / 60000 (50%)]\t1.595719\n",
      "Train Epoch: 1 [32000 / 60000 (53%)]\t1.561953\n",
      "Train Epoch: 1 [34000 / 60000 (57%)]\t1.575946\n",
      "Train Epoch: 1 [36000 / 60000 (60%)]\t1.546252\n",
      "Train Epoch: 1 [38000 / 60000 (63%)]\t1.598880\n",
      "Train Epoch: 1 [40000 / 60000 (67%)]\t1.584369\n",
      "Train Epoch: 1 [42000 / 60000 (70%)]\t1.531976\n",
      "Train Epoch: 1 [44000 / 60000 (73%)]\t1.579969\n",
      "Train Epoch: 1 [46000 / 60000 (77%)]\t1.643317\n",
      "Train Epoch: 1 [48000 / 60000 (80%)]\t1.547751\n",
      "Train Epoch: 1 [50000 / 60000 (83%)]\t1.542819\n",
      "Train Epoch: 1 [52000 / 60000 (87%)]\t1.555464\n",
      "Train Epoch: 1 [54000 / 60000 (90%)]\t1.540781\n",
      "Train Epoch: 1 [56000 / 60000 (93%)]\t1.549668\n",
      "Train Epoch: 1 [58000 / 60000 (97%)]\t1.573142\n",
      "\n",
      "Test set: Average loss: 0.0151, Accuracy: 9510 / 10000 (95%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def test():\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    losses = []  # List to store test losses for each epoch\n",
    "    with torch.no_grad():\n",
    "        for data, target in loaders['test']:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            loss = loss_fn(output, target)\n",
    "            test_loss += loss.item()\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "            losses.append(loss.item())  # Append the test loss for this batch\n",
    "    test_loss /= len(loaders['test'].dataset)\n",
    "    print(f'\\nTest set: Average loss: {test_loss:.4f}, Accuracy: {correct} / {len(loaders[\"test\"].dataset)} ({100. * correct / len(loaders[\"test\"].dataset):.0f}%)\\n')\n",
    "    return losses\n",
    "\n",
    "# Training and testing the NewNet model\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "\n",
    "for epoch in range(1, 2):\n",
    "    train_loss = train(epoch)\n",
    "    test_loss = test()\n",
    "    train_losses.extend(train_loss)\n",
    "    test_losses.extend(test_loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76a049e6",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f8faec11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: 7\n"
     ]
    }
   ],
   "source": [
    "new_model.eval()\n",
    "data, target = next(iter(test_data))\n",
    "data = data.to(device)\n",
    "output = new_model(data)\n",
    "prediction = output.argmax(dim=1, keepdim=True).squeeze().cpu().numpy()\n",
    "print(f'Prediction: {prediction}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8dcd78e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: 7\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "data, target = next(iter(test_data))  \n",
    "data = data.to(device)\n",
    "output = model(data)\n",
    "prediction = output.argmax(dim=1, keepdim=True).squeeze().cpu().numpy()\n",
    "print(f'Prediction: {prediction}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3ccb37a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR: 0.001, Batch Size: 32, Epochs: 5, Test Loss: 0.0014946893572807313, Accuracy: 96.69\n",
      "LR: 0.001, Batch Size: 32, Epochs: 10, Test Loss: 0.0014895165324211122, Accuracy: 97.18\n",
      "LR: 0.001, Batch Size: 64, Epochs: 5, Test Loss: 0.001492116641998291, Accuracy: 96.87\n",
      "LR: 0.001, Batch Size: 64, Epochs: 10, Test Loss: 0.0014882129073143006, Accuracy: 97.28\n",
      "LR: 0.01, Batch Size: 32, Epochs: 5, Test Loss: 0.0017133501887321473, Accuracy: 74.78\n",
      "LR: 0.01, Batch Size: 32, Epochs: 10, Test Loss: 0.0015731307029724121, Accuracy: 88.81\n",
      "LR: 0.01, Batch Size: 64, Epochs: 5, Test Loss: 0.0016039026021957398, Accuracy: 85.72\n",
      "LR: 0.01, Batch Size: 64, Epochs: 10, Test Loss: 0.0016185505032539369, Accuracy: 84.26\n"
     ]
    }
   ],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "        self.fc1 = nn.Linear(320, 50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "        x = x.view(-1, 320)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        return F.softmax(x, dim=1)\n",
    "\n",
    "# Function to train the model\n",
    "def train(model, train_loader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    return running_loss / len(train_loader)\n",
    "\n",
    "# Function to evaluate the model\n",
    "def test(model, test_loader, criterion, device):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += criterion(output, target).item()\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    accuracy = 100. * correct / len(test_loader.dataset)\n",
    "    return test_loss, accuracy\n",
    "\n",
    "# Function to conduct hyperparameter experimentation\n",
    "def hyperparameter_experiment(train_loader, test_loader, lr_values, batch_sizes, epochs, device):\n",
    "    results = []\n",
    "    for lr in lr_values:\n",
    "        for batch_size in batch_sizes:\n",
    "            for epoch in epochs:\n",
    "                model = Net().to(device)\n",
    "                optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "                criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "                train_losses = []\n",
    "                for _ in range(epoch):\n",
    "                    train_loss = train(model, train_loader, optimizer, criterion, device)\n",
    "                    train_losses.append(train_loss)\n",
    "\n",
    "                test_loss, accuracy = test(model, test_loader, criterion, device)\n",
    "\n",
    "                result = {\n",
    "                    'learning_rate': lr,\n",
    "                    'batch_size': batch_size,\n",
    "                    'epochs': epoch,\n",
    "                    'test_loss': test_loss,\n",
    "                    'accuracy': accuracy\n",
    "                }\n",
    "                results.append(result)\n",
    "                print(f\"LR: {lr}, Batch Size: {batch_size}, Epochs: {epoch}, Test Loss: {test_loss}, Accuracy: {accuracy}\")\n",
    "\n",
    "    return results\n",
    "\n",
    "lr_values = [0.001, 0.01]\n",
    "batch_sizes = [32,64]\n",
    "epochs = [5,10]\n",
    "\n",
    "train_data = datasets.MNIST(root='data', train=True, transform=ToTensor(), download=True)\n",
    "test_data = datasets.MNIST(root='data', train=False, transform=ToTensor(), download=True)\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=1000, shuffle=False)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "results = hyperparameter_experiment(train_loader, test_loader, lr_values, batch_sizes, epochs, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "fc2da719",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning Rate\tBatch Size\tEpochs\tTest Loss\tAccuracy\n",
      "0.001\t\t32\t\t5\t\t0.0015\t\t96.69%\n",
      "0.001\t\t32\t\t10\t\t0.0015\t\t97.18%\n",
      "0.001\t\t64\t\t5\t\t0.0015\t\t96.87%\n",
      "0.001\t\t64\t\t10\t\t0.0015\t\t97.28%\n",
      "0.01\t\t32\t\t5\t\t0.0017\t\t74.78%\n",
      "0.01\t\t32\t\t10\t\t0.0016\t\t88.81%\n",
      "0.01\t\t64\t\t5\t\t0.0016\t\t85.72%\n",
      "0.01\t\t64\t\t10\t\t0.0016\t\t84.26%\n"
     ]
    }
   ],
   "source": [
    "print(\"Learning Rate\\tBatch Size\\tEpochs\\tTest Loss\\tAccuracy\")\n",
    "for result in results:\n",
    "    print(f\"{result['learning_rate']}\\t\\t{result['batch_size']}\\t\\t{result['epochs']}\\t\\t{result['test_loss']:.4f}\\t\\t{result['accuracy']:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5913cfad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
